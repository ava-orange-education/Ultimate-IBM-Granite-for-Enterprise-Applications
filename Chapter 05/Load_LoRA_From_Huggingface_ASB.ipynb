{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpdG2lh-Sr_L"
   },
   "source": [
    "## Loading the Fine-tuned Model  \n",
    "  \n",
    "We now demonstrate how to load a model back for inference. This step is crucial for real-world applications where you want to use your trained model without going through the training process again.  \n",
    "  \n",
    "Loading a saved model is typically much faster than training from scratch, making it efficient for deployment scenarios. We'll show how to load both the model and the tokenizer, ensuring that we have all the components necessary for text generation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yusdD7EHSicY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers>=4.45.2 in /home/alan/.local/lib/python3.9/site-packages (4.47.1)\n",
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "     |████████████████████████████████| 374 kB 2.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/alan/.local/lib/python3.9/site-packages (from transformers>=4.45.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/alan/.local/lib/python3.9/site-packages (from transformers>=4.45.2) (0.27.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/alan/.local/lib/python3.9/site-packages (from transformers>=4.45.2) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/alan/.local/lib/python3.9/site-packages (from transformers>=4.45.2) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/alan/.local/lib/python3.9/site-packages (from transformers>=4.45.2) (0.4.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib64/python3.9/site-packages (from transformers>=4.45.2) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/alan/.local/lib/python3.9/site-packages (from transformers>=4.45.2) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/alan/.local/lib/python3.9/site-packages (from transformers>=4.45.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/alan/.local/lib/python3.9/site-packages (from transformers>=4.45.2) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alan/.local/lib/python3.9/site-packages (from transformers>=4.45.2) (24.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/alan/.local/lib/python3.9/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/alan/.local/lib/python3.9/site-packages (from peft) (1.2.1)\n",
      "Requirement already satisfied: psutil in /usr/lib64/python3.9/site-packages (from peft) (5.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/alan/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.45.2) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/alan/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.45.2) (2024.9.0)\n",
      "Requirement already satisfied: networkx in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/alan/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/alan/.local/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alan/.local/lib/python3.9/site-packages (from requests->transformers>=4.45.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.9/site-packages (from requests->transformers>=4.45.2) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alan/.local/lib/python3.9/site-packages (from requests->transformers>=4.45.2) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alan/.local/lib/python3.9/site-packages (from requests->transformers>=4.45.2) (1.26.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alan/.local/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers>=4.45.2\" peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuDMRmoQTAxr"
   },
   "source": [
    "### Loading the Model from Huggingface Hub  \n",
    "  \n",
    "Once a model is pushed to the Hugging Face Hub, loading it for inference or further fine-tuning becomes remarkably straightforward. This ease of use is one of the key advantages of the Hugging Face ecosystem.  \n",
    "  \n",
    "We'll show how to load our fine-tuned model directly from the Hugging Face Hub using just a few lines of code. This process works not only for our own uploaded models but for any public model on the Hub, demonstrating the power and flexibility of this approach.  \n",
    "  \n",
    "Loading from the Hub allows you to:  \n",
    "1. Quickly experiment with different models  \n",
    "2. Easily integrate state-of-the-art models into your projects  \n",
    "3. Ensure you're using the latest version of a model  \n",
    "4. Access models from various devices or environments without needing to manually transfer files  \n",
    "  \n",
    "This capability is particularly useful in production environments, where you might need to dynamically load or update models based on specific requirements or performance metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2-h-R1vrSknZ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e97ffedd06f4fbb900a308544f2c8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/785 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3128ca04cedb47bfa6c12f8fddcb9598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/29.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186955ad706a44ec9b0a818f7c699737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae06b4a63b74a628c77b50edde61e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940b78c55017498685656e79e0008cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276d6f371856407e8e2ab8c5245ff127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f368609b49cb486a8cc5eaef11dfb13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/769 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677cd75ad1b744d5827026b41d027f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92d1a2c6500422c97ac5058533a9391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b891e0c482544d67b50704b7cef6243d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fb018baef94741be6b6fe9428ca5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adef8fbe23d40fcbb60b11e3a0e9445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff543a6b8e014bed8b4c1143e575fb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fb449b26594582b5b36d6bcd99ac28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model_name = \"ibm-granite/granite-3.0-2b-instruct\"\n",
    "adapter_model_name = \"rawkintrevo/granite-3.0-2b-instruct-pirate-adapter\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, adapter_model_name, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXIIhWfPTHkp"
   },
   "source": [
    "## Evaluation  \n",
    "  \n",
    "Just as we did in the fine tuning example, we'll evaluate our fine-tuned model by presenting it with the 'inheritance' prompt we used in the sanity check. This comparison will reveal how the model's output is now 'more piratey'.  \n",
    "  \n",
    "This step demonstrates the power of transfer learning and domain-specific fine-tuning in natural language processing, showing how we can adapt a general-purpose language model to specialized tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kBuzWbI4Tgvz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user>What does 'inheritance' mean?\n",
      "<|assistant|>\n",
      "Ahoy, matey! In the realm of programming, inheritance be a powerful tool, aye. It be a way for a new class to inherit the properties and methods of an existing class, like a ship inheritin' the sails and oars of its predecessor. This be allowin' the new class to build upon the existing class, addin' its own features and behaviors, like a captain addin' his own touch to a ship. Inheritance be a key concept in object-oriented programming, helpin' to create a hierarchical structure of classes, like a family tree of objects. It be a way to reuse code and reduce redundancy, like a ship usin' the same sails for different voyages. Inheritance be a mighty force in the world of programming, aye!<\n"
     ]
    }
   ],
   "source": [
    "input_text = \"<|user>What does 'inheritance' mean?\\n<|assistant|>\\n\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\") # ASB changed from CUDA to CPU 22nd January, 2025\n",
    "stop_token = \"<|endoftext|>\"\n",
    "stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "outputs = model.generate(**inputs, max_new_tokens=500, eos_token_id=stop_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
